一、SparkSql
from pyspark import SparkContext
sc = SparkContext(appName='SparkSql')

from pyspark.sql import *
sqlContext = SQLContext(sc)

#封装的生成table的函数
def registerTable(contentObj, schmeaList, tableName):
    lines = sc.parallelize(contentObj)
    parts = lines.map(lambda l: tuple(l.split(',')))
    
    fields = [StructField(field_name, StringType(), True) for field_name in schmeaList]
    schema = StructType(fields)

    schemaTable = sqlContext.applySchema(parts, schema)
    schemaTable.registerTempTable(tableName)

#初始化数据
def initialTable():
    registerTable(['a,10', 'b,20', 'c,5', 'c,10'], 'name age'.split(), 'peopleAge')
    registerTable(['a,SE', 'b,QE'], 'name career'.split(), 'peopleCareer')
 
#filter查询 返回Row结构 相当于哈希表
def with_filter():
    results = sqlContext.sql('SELECT * FROM peopleAge where age>10')
    for i in results.collect():
        print i

#groupby查询
def with_aggr():
    results = sqlContext.sql('SELECT name,sum(age) FROM peopleAge group by name')
    for i in results.collect():
        print i

#join查询
def with_join():
    results = sqlContext.sql('SELECT a.name,age,career FROM peopleAge a join peopleCareer b on a.name=b.name')
    for i in results.collect():
        print i

if __name__ == '__main__':
    initialTable()
    #with_filter()
    #with_aggr()
    with_join()

二、spark本地运行
spark-submit --master local ml.py 
